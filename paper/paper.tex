% This project is part of the Continuous-Function Estimator project
% Copyright 2019 the authors.

% to-do
% -----
% - get yourself an ORCID and use it.
% - Is ``vectorized'' the right word? It's what I say, but now I'm doubting it. We could go with linear-regression. Or affine-invariant. Or unbinned. Or continuous-function?

% style notes
% -----------
% - line break at sentence breaks? Or more frequently? So git diff works good?

%\documentclass[twocolumn]{aastex62}
%\documentclass[12 pt]{article}
\documentclass[modern]{aastex62}

\usepackage[sort&compress]{natbib}
\usepackage{graphicx}
\graphicspath{ {./images/} }
\usepackage{xspace}
\usepackage{xcolor}


% aastex parameters
%%\hypersetup{linkcolor=red,citecolor=green,filecolor=cyan,urlcolor=magenta}
\received{XXX}
%\revised{not yet}
\accepted{YYY}
%\submitjournal{ApJ}
\shorttitle{A Continuous Correlation Function Estimator}
\shortauthors{Storey-Fisher and Hogg}

% language
\newcommand{\cf}{2pcf\xspace} %2pF? 2PCF? %TODO: fix spacing after
\newcommand{\Est}{The Continuous-Function Estimator\xspace}
\newcommand{\est}{the Continuous-Function Estimator\xspace}
\newcommand{\LS}{LS\xspace}
\newcommand{\foreign}[1]{\textsl{#1}}
\newcommand{\etc}{\foreign{etc}}
% math
\newcommand{\inv}{^{-1}}
\newcommand{\T}{^{\mathsf{T}}}
\newcommand{\hmpc}{$h^{-1}$Mpc}
% comments
\newcommand{\KSF}[1]{\textcolor{teal}{KSF says: #1}}


% margins
%\addtolength{\topmargin}{-0.75in}
%\addtolength{\textheight}{1.50in}

% affiliations
\newcommand{\ccpp}{\affiliation{%
    Center for Cosmology and Particle Physics,
    Department of Physics,
    New York University}}
\newcommand{\flatiron}{\affiliation{%
    Flatiron Institute, Simons Foundation}}
\newcommand{\cds}{\affiliation{%
    Center for Data Science,
    New York University}}
\newcommand{\mpia}{\affiliation{%
    Max-Planck-Institut f\"{u}r Astronomie, Heidelberg}}


\begin{document}\sloppy\sloppypar\raggedbottom\frenchspacing

\title{\textbf{Two-point statistics without bins: A continuous-function generalization of the correlation function estimator for large-scale structure}}
%Two-point statistics without bins: A correlation function estimator for large-scale structure in a continuous-function basis
%Two-point statistics without bins: The correlation function estimator for large-scale structure generalized to a continuous-function basis
%\title{\textbf{A Continuous-Function Estimator of the Correlation Function for Large-Scale Structure}}
%\title{\textbf{A Continuous Correlation Function Estimator for Large-Scale Structure}}
%\title{\textbf{A Continuous-Function Correlation Function Estimator for Large-Scale Structure}}
%\title{\textbf{Projecting the Correlation Function onto Continuous Functions for Large-Scale Structure Surveys}
%\title{\textbf{A Continuous Representation of the Correlation Function Estimator for Large-Scale Structure}}
%\title{\textbf{Correlation Function Estimation with Continuous Functions for Large-Scale Structure}}
%\title{\textbf{Estimating the Two-Point Correlation Function with Continuous Functions}}
%\title{\textbf{Projecting Two-Point Correlations onto General Basis Functions: A New Estimator For Large-Scale Structure}}
%\title{\textbf{A Correlation Function Estimator with General Basis Functions for Large-Scale Structure}}
%\title{\textbf{A Generalized Correlation Function Estimator For Large-Scale Structure}}
%\title{\textbf{No More Bins:\\ A Vectorized Correlation Function Estimator For Large-Scale Structure}}
%\title{A Generalized Correlation Function Estimator for Galaxy Surveys}

%Generalized Estimator?
%Projected Estimator?
%Continuous Estimator?
%Continuous-Function Estimator?
%Linear-Regression Estimator?
%Vectorized estimator?

\author[0000-0001-8764-7103]{Kate Storey-Fisher}
\ccpp

\author[0000-0003-2866-9403]{David W. Hogg}
\ccpp
\cds
\mpia
\flatiron

\begin{abstract}\noindent
% Context
The two-point correlation function (\cf) is the most important statistic in structure formation.
Current estimators of the \cf, which is used to measure the clustering of density field tracers, have significant limitations.
The standard Landy-Szalay (\LS) estimator evaluates the \cf in bins of radial separation between objects, and the choice of bins introduces a trade-off between bias and variance. %is radial the right word? does that imply line-of-sight?
% Aims
We present a new, generalized estimator, \est, for the \cf that obviates binning in separation or any other property.
% Results
\Est replaces the binned pair counts of \LS with a continuous-function representation in the form of a linear superposition of basis functions.
These functions can take advantage of the known form of the \cf, and can depend on other properties of the pairs in addition to their separation.
Inspired by linear least-squares fitting, \est marks the numerator terms of \LS with vectors of these functions, and the denominator with a tensor of their outer products.
The estimator outputs the best-fit linear combination of basis functions to describe the \cf, in the same limit in which \LS is justifiably optimal for binned applications. 
% Results
We demonstrate that it can estimate the clustering of artificial data in representations that provide more accuracy with fewer basis functions than \LS, thus reducing requirements on mocks for covariance estimation.
We apply \est to the Luminous Red Galaxy sample of the Sloan Digital Sky Survey (SDSS), and show that it can extract a compact set of Fisher-optimal statistics for measuring the location of the baryon acoustic feature.
We discuss other applications and limitations of \est for present and future studies of large-scale structure, including determining the dependence of clustering on galaxy properties and potentially unifying real-space and Fourier-space approaches by using a Fourier basis representation.
\end{abstract}

\keywords{cosmology: large-scale structure --- galaxies: statistics} %?

\section{Introduction}

The large-scale structure (LSS) of the universe is a critical probe of fundamental cosmology. 
It encodes information about the physics of the early universe and the subsequent expansion history.
In particular, the LSS provides a probe of the Baryon Acoustic Oscillations (BAO), density fluctuations resulting from baryon-photon coupling in the early universe.
The distance traveled by these waves imprints a feature on the statistical description of the LSS, which can be used to determine the characteristic BAO length scale \citep{EisensteinHu1998}.
The LSS also contains the signature of redshift-space distortions caused by the peculiar velocities of galaxies, which probe the growth rate of structure \citep{Kaiser1987}.
Additionally, the LSS can be used to constrain galaxy formation in conjunction with models of galaxy bias (e.g. \citealt{Hamilton1988}). %?
With current observations, the LSS is well-described by a cold dark matter model with a cosmological constant, the standard $\Lambda$CDM model.
Upcoming galaxy surveys will observe larger volumes with improved measurements, allowing us to test $\Lambda$CDM to even higher precision.

We characterize the LSS by using luminous sources to trace the underlying matter density field.
These tracers are often to taken to be galaxies, but can also be galaxy clusters, quasars and other sources.
% should i say that hereafter we will take them to be galaxies?
The clustering of these objects is measured with two-point statistics, namely the power spectrum $P(k)$ and the two-point correlation function (\cf). %name xi_r?
These characterize the clustering in Fourier space and real space, respectively, with the \cf defined as the Fourier Transform of the power spectrum: 
%TODO: CHECK, PROBS WRONG
\begin{equation}
\xi(r) = \frac{1}{2\pi} \int_0^{\infty} P(k) d^3k
\end{equation}
In principle, the power spectrum and the two-point correlation function contain the same information.
However, in practical applications the survey boundaries introduce nontrivial issues in computing these statistics, leading to diverging approaches to their computation with a significant difference in expense.
The \cf requires more computational power and extra survey products, but it is an incredibly useful tool; for instance, it well-suited to the analysis of the BAO feature which manifests at a single scale in real space.

The \cf measures the excess probability that any two galaxies are separated by a given distance, compared to a uniform distribution; effectively, it characterizes the strength of clustering at a given spatial scale. 
% need to discuss statistical homogeneity & isotropy? (ergodicity?)
In calculating the \cf, the boundaries of the surveys prevent us from directly summing pair counts due to nontrivial edge effects.
To account for the survey boundaries as well as corrupted regions due to issues such as bright foreground stars, a set of random points are Poisson-distributed within the acceptable survey window. 
The pairwise correlations of these points, which by construction are unclustered, are used to normalize out the survey window when estimating the \cf of the clustered data.
Typically, this requires random points on the order of 10-100 times the number of data points, making the random correlations the limiting factor in \cf computation.

The \cf is computed in bins of radial separation, meaning that in practice it measures the volume average of the \cf over the bin. %is this accurate?
The choice of bins requires a trade-off between bias and variance: fewer bins may bias the result, while more bins increases the variance of measurement.
Finite-width bins also result in a loss of information about the property in which one is binning. 
Especially as we strive for extreme precision in large-scale structure analyses, we should be maximizing the information we extract from the data.
Moreover, we note that standard estimators are fundamentally biased due to this finite binning; we will address this in a future work. %TODO: figure out this wording. need to explain more than this or leave out entirely?

%some papers have equations (even this eqn) in intro - but should bump it to motivation and just explain?
The current standard estimator was proposed by \cite{LandySzalay1993}, hereafter \LS. It is based on summing all data pairs $DD$ with a given separation and using data-random pairs $DR$ and random pairs $RR$ to correct for the survey boundary. The correlation function $\xi_k(r)$ for the $k^\mathrm{th}$ separation bin is
%assume we should keep this notation even though it's clunky, as it's recognizable
%should we use estimator notation with a hat on xi?
\begin{equation}
\xi_k(r) = \frac{DD_k(r) - 2DR_k(r) + RR_k(r)}{RR_k(r)},
\end{equation}
where we assume the pair counts are normalized. %should we write out normalization explicitly? here or somewhere else?
Compared with other estimators based on simple combinations of $DD$, $DR$ and $RR$, \LS has been shown to have the lowest bias and variance \citep{Kerscher2000}. %added simple because there was the recent paper which looked at all linear combos and optimized, think that one might have better bias/variance properties
Estimators of the \cf must also take into account the imperfect nature of the survey area that is in the window, including the target completeness and fiber collisions; typically each galaxy pair is assigned a weight based on these. %is this necessary? not sure where it fits
Then, pair counts are replaced by the sum of pair weights.

Variations on the random catalog pair count method have been proposed in recent years.
\cite{Demina2016} replaced the $DR$ and $RR$ terms with an integral over the probability map, reducing computation time and increasing precision.
An estimator proposed by \cite{VargasMagana2013} iterates over sets of mock catalogs to find an optimal linear combination of data and random pair counts, reducing the bias and variance.
The marked correlation function \citep{WhitePadmanabhan2009} sums weights based on properties of the galaxies one is interested in, such as the local density or galaxy luminosity, and avoids the use of a random catalog.
The estimators described so far have all taken probabilistic approaches; some have also taken a likelihood approach.
\cite{BaxterRozo2013} introduced a maximum likelihood estimator for the \cf, which achieves lower variance compared to the \LS estimator, enabling finer binning and requiring a smaller random catalog for the same precision.

These estimators present improvements to \LS, but they all still require binning in separation.
Some require additional computational costs or layers of complexity, so the standard formulation of \LS continues to be the default estimator used in most analyses. %make sure this is true! talk to cosmologists who do these analyses

In this paper, we present a new estimator for the correlation function, \est, which generalizes the \LS estimator to produce a continuous estimation of the \cf. 
\Est projects the galaxy pairs onto a set of continuous basis functions and computes the best-fit linear combination of these functions.
The basis representation can depend on the pair separation as well as other desired properties, and can also utilize the known form of the \cf.
For top-hat basis functions, \est exactly reduces to the \LS estimator. 
This estimator removes the need for binning and allows for the \cf to be represented by fewer basis functions, requiring fewer mock catalogs to compute the covariance matrix.
It is particularly well-suited to the analysis of LSS features such as the BAO peak; we find that we can more accurately locate the peak with fewer components.

%will obviously keep changing this as paper develops
%spell out acronyms again that were defined in the abstract?
This paper is organized as follows. 
In Section \ref{sec:motiv}, we motivate and derive our estimator. 
In Section \ref{sec:est}, we prove its correctness and describe our implementation, and demonstrate its application on a simulated data set. 
Our results on the SDSS LRG sample are shown in Section \ref{sec:app}. 
We discuss the implications and other possible applications in Section \ref{sec:discuss}. 

\section{Motivation} \label{sec:motiv}

The pair counts for \LS and related estimators are defined explicitly as
%TODO: fix spacing on sides of equal sign
\begin{eqnarray}\displaystyle
DD_k &\equiv& \sum_{n n'} i(g_k < |x_n - x_{n'}| < h_k) \\
DR_k &\equiv& \sum_{n m} i(g_k < |x_n - x_m| < h_k) \\
RR_k &\equiv& \sum_{m m'} i(g_k < |x_m - x_{m'}| < h_k),
\end{eqnarray}
where $DD_k$ is the count of data--data tracer pairs in bin $k$ (which has bin edges $g_k$ and $h_k$), $i()$ is an indicator function, $x$ is the tracer position, the $n$ and $n'$ indices index data positions, the $m$ and $m'$ indices index random catalog positions, $DR_k$ is the count of data--random pairs, and $RR_k$ is the count of random--random pairs.

The Landy-Szalay estimator is justified by\ldots

Estimating clustering is closely related to least-squares fitting.
We are trying to find the best representation of spatial data in the space of two-point radial separation.
Recall that the linear least-squares fit to a set of data 
\begin{equation}
X = [A\T C\inv A]\inv [A\T C\inv Y]
\end{equation}
where $X$ is the vector of best-fit parameters, $A$ is a matrix with zeroth and first order terms of $x$ data, $C$ is the covariance matrix, and $Y$ is a column vector of $y$ data.
The second bracketed term contains the observed data; the first bracketed term weights the data by the errors.
Naively, in the case of the \cf, the observed data is the pair counts at a given separation, and the weights are provided by the pair counts of the random catalog.
Indeed, this is reminiscent of the so-called natural estimator of the \cf, $\xi_k = DD_k/RR_k - 1$ (e.g. \citealt{Kerscher2000}).

From this connection, we can infer the form of the estimator.

% TODO: fix this capitalization issue
\section{\Est} 
\label{sec:est}

We generalize the \LS estimator defined above to any set of $K$ basis functions $f$ of the pair, so we now have
\begin{eqnarray}\displaystyle
DD &\equiv& \sum_{n n'} f(T_n, T_{n'}) \\
DR &\equiv& \sum_{n m} f(T_n, T_{m}) \\
RR &\equiv& \sum_{m m'} f(T_m, T_{m'}) \\
QQ &\equiv& \sum_{m m'} f(T_m, T_{m'}) \cdot f\T(T_m, T_{m'}),
\end{eqnarray}
%introduced this T notation to be the most general but it's a bit clunky (see below)
%what do we think about K-vector notation?
%matrix->tensor?
where $DD$, $DR$, and $RR$ are now $K$-vectors, $T$ refers to the data for the given tracer, and $QQ$ is a $K$-by-$K$ matrix defined as the outer product of basis function evaluations of the random--random pairs.

Then, we can compute the \cf as
\begin{eqnarray}\displaystyle
% here is where the T notation is weird - when you look at evaluation
\xi(T_l, T_{l'}) &\equiv& a\T \cdot f(T_l, T_{l'}) \\
a &\equiv& QQ\inv \cdot (DD + 2DR - RR) 
\end{eqnarray}
where $T_l$ and $T_{l'}$ contain the data values at which to evaluate $\xi$, and $a$ is a $K$-vector of the ``amplitudes'' of the basis functions.
This generalized two-point estimator, or \est, removes the need for binning as the basis functions can be continuous.
It is invariant under affine transformations; we show this in Appendix \ref{sec:affine}.

%%% INDEX NOTATION VERSION %%%
\iffalse % Messier and maybe inaccurate (with the inverse QQ)? but more consistent
\begin{eqnarray}\displaystyle
DD_k &\equiv& \sum_{n n'} f_k(T_n, T_{n'}) \\
DR_k &\equiv& \sum_{n m} f_k(T_n, T_{m}) \\
RR_k &\equiv& \sum_{m m'} f_k(T_m, T_{m'}) \\
QQ_{kl} &\equiv& \sum_{m m'} f_k(T_m, T_{m'}) f_l(T_m, T_{m'}),
\end{eqnarray}
%should we change the letter of k to mean basis functions rather than bins now?
%better notation for T?
Then, we can compute the \cf as
\begin{eqnarray}\displaystyle
\xi_k(T_j, T_{j'}) &\equiv& a_k f_k(T_j, T_{j'}) \\
a_k &\equiv& [QQ\inv]_{kl} (DD_l + 2DR_l - RR_l) 
\end{eqnarray}
where $k$ now indexes the basis functions, $T$ refers to all of the data for the given tracer, and $QQ_{kl}$ is the product of basis function indexed by $k$ and $l$ for the random--random pairs.
\fi %%%%%%%%%%%%%%%%%%

In the case where the basis functions only depend on the pair separation, we have $f = f(|x_n - x_{n'}|)$ and $\xi = \xi(r)$, where $r$ is the separation at which to evaluate the correlation function.
Specifically, we can choose to define a set of basis functions $f$ as,
\begin{equation}
f_k(|x_n - x_n'|) =  i(g_k < |x_n - x_{n'}| < h_k)
\end{equation}
where $k$ denotes a particular bin in separation.
In this case the $DD$, $DR$ and $RR$ vectors become binned pair counts and the $QQ$ matrix becomes diagonal, with diagonal elements equal to the $RR$ vector elements. %TODO CONTINUE HERE
Then for this function $f$, which is the definition of a top-hat (rectangular) function, the estimator is equivalent to that of Landy-Szalay.

We prove that the estimator is invariant under affine transformations.

We show that in the limit of infinitesimal bins\ldots

These show that the estimator is correct.

We implement this estimator based on the correlation function package \texttt{corrfunc} by \citep{citeXXX} \ldots
The code is open-source and available at XXX.


\subsection{Demonstration using Spline Basis Functions}
\label{sec:spline}

We demonstrate the application of the \est on a simulated Gaussian random field.
We generate lognormal mock catalogs using the \texttt{nbodykit} package.
We use an input power spectrum with the Planck cosmology \KSF{CITE}.
The true correlation function is then known: it is the Fourier transform of the input power spectrum, computed numerically.
Our test catalogs have size (750 \hmpc)$^3$ and a galaxy number density of $3 \times 10^{-4}$, to match that of the Brightest Cluster Galaxies \KSF{CITE}. 
We compute 100 realizations of this box. 
We also generate a uniformly sampled random catalog with 10 times the number of galaxies as in the data catalogs.

\label{fig:quadratic}
\begin{figure}[h]
\centering
    \includegraphics[width=0.8\textwidth]{tophat_quadratic}
    \caption{A comparison between the standard tophat estimator (blue solid) and the \est with a quadratic spline basis function (red dotted). The shaded region is the 1$\sigma$ variation in the 100 mock catalogs. The quadratic estimator has lower bias and lower variance with fewer bins.}
\end{figure}

\KSF{Should i have a figure showing the basis functions?}

\KSF{These sims have the weird thing where the data says the peak is offset. Should figure out, and also it's distracting from the main point of the figure}

\KSF{FINAL PARAMS:  number density 3e-4 (now 1e-4). rmax 200 hMpc (now 150). rcont=1000 (now rcont=300). L=1000 hMpc (now 750 - need more to account for higher rmax)}

\KSF{should RMSE be shown in figure as well as described in text?}

We compare the standard estimator, reformulated as continuous functions using a tophat basis, to the \est with a quadratic spline basis.
We use second-order B-splines, which constitute the set of basis functions for a quadratic spline \KSF{CITE}.
For the tophat basis we use 44 basis functions in the range $40 < r < 150$ \hmpc, each with a width of 2.5 \hmpc. 
For the quadratic spline basis, we use the same $r$ range, but with only 11 basis functions, and knots chosen on a grid of 10 \hmpc.
\KSF{need to specify more about knots? is this accurate (not quite, the knots repeat values at the edge i think...) and should i mention control points?}

The results are shown in Figure~\ref{fig:quadratic}, compared to the true input \cf.
The quadratic basis results in a \cf estimate that has a root mean square error (RMSE) with respect to the truth of $4.39 \times 10^{-4}$, compared to $5.43 \times 10^{-4}$ for the tophat basis.
This holds true when we average over the bin \KSF{need to do weighted average based on which r-values contribute? supposed to hear from tinker}, as in standard practice, and then compute the RMSE.
The quadratic basis also results in lower variance across the 100 mocks, compared to the tophat basis.
Thus, the \est allows for a more accurate and precise estimate of the \cf with fewer components.


\subsection{BAO Scale Estimation Test}

Measurement of the BAO scale provides a good use case for our estimator.

The BAO peak position is a function of multiple distance scales.
We can measure it by comparing the data to a fiducial model of clustering.
To a very good approximation, the peak position depends on the single scale dilation parameter
\begin{equation}
\alpha = \Bigg( \frac{D_A(z)}{D_A^{mod}(z)} \Bigg)^{2/3} \Bigg( \frac{H^{mod}(z)}{H(z)} \Bigg)^{1/3} \Bigg( \frac{r_s^{mod}}{r_s} \Bigg),
\end{equation}
where $D_A$ is the angular diameter distance, $H$ is the Hubble constant, $r_s$ is the sound horizon scale at the drag epoch, and the superscript ``$mod$'' denotes the value for the fiducial model.
If we find $\alpha>1$, the peak is shifted to smaller scales than its position in the model, and if $\alpha<1$, the peak is shifted to larger scales.

For BAO measurements, we measure the spherically averaged redshift space correlation function, $\xi(s)$.
This multipoles of the correlation function are given by
\begin{equation}
\xi_l(s) = \frac{1}{2} \int^{1}_{-1} \xi_l(s, \mu) L_l(\mu) d\mu
\end{equation}
where $L_l$ is the Legendre multipole of order $l$,  $s$ is the redshift-space separation between pairs, and $\mu = cos(\theta)$, $\theta$ is the angle between the separation vector $s$ and the line-of-sight direction (i.e. $\mu=1$ indicates a galaxy pair along the line of sight, $\mu$=0 indicates a pair with only transverse separation).
The two-point correlation function that we use is the $l=0$ multiple; from now on we will omit the subscript, $\xi(s) = \xi_0(s)$.

In standard practice, a fitting function is used to determine the value of $\alpha$:
\begin{equation}
\xi^{fit}(s) = B^2 \xi^{mod}(\alpha s) + \frac{a_1}{s^2} + \frac{a_2}{s} + a_3
\end{equation}
where $B$ is a constant that allows for a large-scale bias, and $a_1$, $a_2$, and $a_3$ are nuisance parameters to account for the broadband shape.
The model is given by \KSF{DESCRIBE}, and a $\chi^2$ fit is performed with five free parameters: $\alpha$, $B$, $a_1$, $a_2$, and $a_3$.
The resulting value for $\alpha$ is used to derive the actual values of the distance scales of interest.

The form of the standard fitting function is well-suited to our estimator, as it is a few-parameter model with a linear combination of terms.
To use our estimator to estimate $\alpha$, we take the numerical partial derivative of the model with respect to $\alpha$, using a change in $\alpha$ of size $d\alpha$.
Our fitting function is then
\KSF{Check if the derivative part is right!}
\begin{equation}
\xi^{fit}(s) = B^2 \xi^{mod}(s) + C d\alpha\frac{d\xi_{mod}(\alpha s)}{d\alpha} + \frac{a_1}{s^2} + \frac{a_2}{s} + a_3,
\end{equation}
where $C$ describes the contribution of $\alpha$.
That is, a value of $C=0$ indicates that the model is the best fit to the data and no scale dilation is needed.
More generally, we can use $C$ to compute $\alpha$ with $\alpha = 1 + Cd\alpha$.
With the terms in this fitting function the bases of our estimator, the values $B$, $C$, $a_1$, $a_2$, and $a_3$ are then given by the output amplitude vector $a$ as described in $\S$\ref{sec:est}.

\KSF{Choose realistic alpha values}

\KSF{Instead of this alpha test, should do a slightly more realistic one where we change the model cosmology? Issue is that we then don't know the truth}

\KSF{BUT my estimator is having a hard time recovering the truth anyway! Because of variance in the realizations. It converges to the same, but wrong, value of alpha, and diff ones for diff realizations. Need to average over many? (Still maybe wrong due to some persistant weirdness as mentioned above! Maybe need to check how I'm doing the lognormals)}

\label{fig:bao_bases}
\begin{figure}[h]
\centering
    \includegraphics[width=0.8\textwidth]{bao_bases}
    \caption{The set of basis functions used to fit for the BAO scale using our estimator. The $B^2$ term (green) is the fiducial model used to determine the scale dilation parameter $\alpha$. The $C$ term is the derivative of this model with respect to $\alpha$, allowing for the estimation of this parameter. The $a_1$, $a_2$, and $a_3$ terms are nuisance parameters to fit the broadband shape. The \KSF{Make colorblind friendly! Do i also need to use different linestyles? Or maybe weights?}}
\end{figure}

We demonstrate this method using the same set of lognormal mocks as in $\S$\ref{sec:spline}.
We construct a simple test in which our model is the known input model, the Planck parameters, but with some scale dilation offset, here $\alpha_{mod}=1.05$ (while the true value is, of course, $\alpha=1$).
The set of basis functions for our estimator is shown in Figure~\ref{fig:bao_bases}.

We perform an iterative procedure to estimate $\alpha$.
After the first estimation with the $\alpha=1.05$, model, we use the value of $C$ to compute the recovered $\alpha$, as described above.
We then take that value as our $\alpha_{mod}$, and re-run the estimator.
We stop when the percent change between $\alpha_{mod}$ and the recovered $\alpha$ is less than 0.01\%; it typically takes only $\sim$5 iterations to converge.
The resulting estimate for the correlation function is shown in Figure~\ref{fig:bao}, in comparison to the traditional tophat estimator.

\label{fig:bao}
\begin{figure}[h]
\centering
    \includegraphics[width=0.8\textwidth]{bao}
    \caption{Estimation of the correlation function using our estimator with basis functions based on the BAO fitting function (purple dot-dashed). We also show the standard Landy-Szalay estimator, displayed as a tophat function (blue), as well as the true input correlation function (black). \KSF{This is a placeholder figure! Need to resolve issues above.} \KSF{Should this and the basis set figure be a single figure with two panels?}}
\end{figure}

% \section{Application to survey data} \label{sec:app}

% We apply the estimator to the SDSS DR3 sample from \cite{Eisenstein2005}.

% The reconstructed dataset we use is described in \cite{Kazin2010}.

\section{Discussion} \label{sec:discuss}

We estimated the \cf with fewer components to achieve the same level of precision.

\subsection{Beyond the Landy-Szalay Estimator}

We note that our estimator doesn't rely on \LS; it could be implemented for various linear combination of pair counts, given certain properties. %aka the tensor term

We discuss the relation of our estimator to the optimized estimator of \cite{VargasMagana2013}, which also uses a linear combination approach.

\subsection{Computational Performance}

Our performance is by definition the same (marginally more) compared to traditional estimators due to the limiting factor of pair-finding.

\subsection{Computing the Covariance Matrix}

The \est results in \cf estimates that are just as accurate with fewer components.
This is critical when computing the covariance matrix.
The covariance matrix is used to estimate the error on a measurement, by evaluating the \cf on a large number of mock catalogs and computing the covariance between the bins.
In our replacement of bins with continuous functions, the covariance matrix will be the covariance between these functions.
The unbiased estimator for the sample covariance matrix is
\begin{equation}
\hat{C}^{ML}_{ij} = \frac{1}{N_{mocks}-1} \sum_{k=1}^{N_{mocks}} \bigg( x^k_i - \bar{x}_i \bigg) \bigg( x^k_j - \bar{x}_j \bigg),
\end{equation}
where $k$ denotes the index of the mock, $i$ and $j$ denote the index of the bin or component, $x$ denotes the estimate in that bin for that mock, and $\bar{x}$ denotes the mean value of the estimate in that bin across the mocks.
To get an unbiased estimate of the inverse covariance matrix, we require a correction factor, as the inverse of an unbiased estimator is not necessarily unbiased.
The unbiased estimator for the sample inverse covariance matrix can be shown to be \citep{Hartlap2007}
\begin{equation}
\hat{C}\inv = \frac{N_{mocks}-N_{bins}-2}{N_{mocks}-1} \bigg( \hat{C}^{ML}\bigg) \inv.
\end{equation}

%This prefactor correction results in the  propagates to the variance in 
The variance in the elements of this estimator then have a dependence on $N_{mocks}$ and $N_{bins}$.
This propagates to the derived cosmological parameters, resulting in an overestimation of the error bars.
Assuming that $N_{mocks} >> N_{bins}$ (and both much larger than the number of parameters to be estimated), and that the measurements are Gaussian distribued, the error bars are inflated by a factor of $(1 + N_{bins}/N_{mocks})$ (i.e., the true constraints are tighter than the derived ones).
This factor is critical at the precision of cosmological parameter estimation.

Typically, this is dealt with by generating a very large number of mocks.
For the Baryon Oscillation Spectroscopic Survey (BOSS, \citealt{Eisenstein2011}), $\sim$600 mocks were needed and the analysis used 41 bins.
Future surveys will have more costly requirements on mock catalogs, with larger simulations necessary to cover the larger survey volumes.

An alternative to increasing $N_{mocks}$ is decreasing $N_{bins}$ to achieve the same error on precision.
In the standard method, this is shown to \emph{increase} the statistical error, albeit only slightly \cite{Percival2013}.
A substantial increase in bin width would prevent capturing information in finer clustering features; even the relatively broad BAO peak requires a bin size on the order of its width of $\sim$10\hmpc.
In fact, in the standard method more bins would be desireable, but the number is limited by the available number of mocks due to the error discussed above.

With our estimator, we have shown that we can reduce the variance by using fewer components, without sacrificing accuracy.
This means that we can safely reduce $N_{bins}$.
To then achieve the same precision on the error on the cosmological parameters, a lower value of $N_{mocks}$ becomes possible.
This will significantly reduce requirements on mocks, which will be particularly important for upcoming large surveys. 


\subsection{Further Applications}

In the future, we could simultaneously estimate the correlation function and the power spectrum.

We could also investigate: gradients, anomalies, growth of structure, directly estimating the cosmological parameters, calibrating for systematics, clustering dependence galaxy properties


\acknowledgements
It is a pleasure to thank\ldots

\appendix
\section{Affine Invariance}\label{sec:affine}

The estimator is invariant under an affine transformation. We represent this by a matrix M, such that 
\begin{equation}
f' \leftarrow Mf
\end{equation}
Then in the primed basis, the pair counts become
\begin{eqnarray}\displaystyle
DD' &=& \sum_{n n'} M f_{n n'} = M\,DD
\\
DR' &=& \sum_{n m} M f_{n m} = M\,DR
\\
RR' &=& \sum_{m m'} M f_{m m'} = M\,RR
\end{eqnarray}
where $f_{n n'} = f_k(T_n, T_{n'})$.
We have factored $M$ out of the summation. For the $QQ$ matrix we have
\begin{eqnarray}\displaystyle
QQ' &=& \sum_{m m'} (Mf_{m m'}) \cdot (Mf_{m m'})\T \\
&=& M\Bigg[ \sum_{m m'} f_{m m'} \cdot f_{m m'}\T \Bigg]M\T \\
&=& M QQ M\T
\end{eqnarray}
Then the amplitudes in the primed basis become
\begin{eqnarray}\displaystyle
a' &=& [M QQ M\T]\inv \cdot [MDD - 2\,MDR + MRR] \\
&=& (M\T)\inv QQ\inv \cdot [DD - 2\,DR + RR] \\
&=& (M\T)\inv a
\end{eqnarray}
and the estimator in the primed basis is 
\begin{eqnarray}\displaystyle
\xi' &=& [(M\T)\inv a]\T \cdot (Mf) \\
&=& a\T[(M\inv)\T]\T \cdot (Mf) \\
&=& a\T M\inv \cdot Mf \\
&=& a\T \cdot f = \xi.
\end{eqnarray}
Thus after an affine transformation of the basis function, the resulting estimator is equivalent to the estimator in the original basis.

We note that this requires $M$ be invertible.
% not sure if i understand what i wrote here
However, any two equivalent bases must be related by the inverse of a transformation matrix, so this requirement is already satisfied.

%need style file error
%\bibliographystyle{apj} 
\bibliography{paper}

\end{document}
